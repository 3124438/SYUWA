import streamlit as st
import cv2
import numpy as np
import mediapipe as mp
import tensorflow as tf
from tensorflow.keras.layers import Layer
import tensorflow.keras.backend as K
from streamlit_webrtc import webrtc_streamer, VideoTransformerBase
from collections import deque
import av

# =================================================
# âš™ï¸ è¨­å®šã‚¨ãƒªã‚¢
# =================================================
MODEL_FILE_NAME = "best_sign_model.keras"
# ã‚¯ãƒ©ã‚¹åã¯4ã¤ï¼ˆã‚ãªãŸã®ç’°å¢ƒã«åˆã‚ã›ã¦ï¼ï¼‰
CLASS_NAMES = ["Label 1", "Label 2", "Label 3", "å‹•ã‘ï¼ï¼ï¼"] 

# =================================================
# Attentionå±¤
# =================================================
@tf.keras.utils.register_keras_serializable()
class Attention(Layer):
    def __init__(self, **kwargs):
        super(Attention, self).__init__(**kwargs)

    def build(self, input_shape):
        self.W = self.add_weight(name='attention_weight', 
                                 shape=(input_shape[-1], 1), 
                                 initializer='normal', trainable=True)
        self.b = self.add_weight(name='attention_bias', 
                                 shape=(input_shape[1], 1), 
                                 initializer='zeros', trainable=True)
        super(Attention, self).build(input_shape)

    def call(self, x):
        e = K.tanh(K.dot(x, self.W) + self.b)
        a = K.softmax(e, axis=1)
        output = x * a
        return K.sum(output, axis=1)

    def get_config(self):
        config = super(Attention, self).get_config()
        return config

@st.cache_resource
def load_model():
    return tf.keras.models.load_model(MODEL_FILE_NAME, custom_objects={'Attention': Attention})

try:
    model = load_model()
except Exception as e:
    st.error(f"ã‚¨ãƒ©ãƒ¼: {e}")
    model = None

# MediaPipeè¨­å®š
mp_holistic = mp.solutions.holistic
mp_drawing = mp.solutions.drawing_utils

# =================================================
# ğŸ›ï¸ UIã‚µã‚¤ãƒ‰ãƒãƒ¼è¨­å®š
# =================================================
st.sidebar.title("è¨­å®šãƒ‘ãƒãƒ«")
# ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰ã®åˆ‡ã‚Šæ›¿ãˆã‚¹ã‚¤ãƒƒãƒ
DEBUG_MODE = st.sidebar.checkbox("ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰ï¼ˆéª¨æ ¼è¡¨ç¤ºï¼‰", value=True)
st.sidebar.write("---")
st.sidebar.write("æ˜ åƒã®å³å´ã«ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ åˆ†æçµæœãŒè¡¨ç¤ºã•ã‚Œã¾ã™ã€‚")

# ------------------------------------------------
# æ˜ åƒå‡¦ç†ã‚¯ãƒ©ã‚¹
# ------------------------------------------------
class VideoProcessor(VideoTransformerBase):
    def __init__(self):
        self.sequence = deque(maxlen=30)
        self.holistic = mp_holistic.Holistic(
            min_detection_confidence=0.5,
            min_tracking_confidence=0.5
        )
        # åˆæœŸå€¤
        self.probs = np.zeros(len(CLASS_NAMES)) 
        self.result_label = "Waiting..."
        self.result_conf = 0.0
        self.status_text = "Init..."
        
        # UIã‹ã‚‰å—ã‘å–ã£ãŸè¨­å®šï¼ˆã‚¯ãƒ©ã‚¹ä½œæˆæ™‚ã«æ¸¡ã›ãªã„ã®ã§ã€globalå¤‰æ•°ã‚’å‚ç…§ã™ã‚‹å½¢ã‚’ã¨ã‚Šã¾ã™ï¼‰
        self.debug = DEBUG_MODE

    def transform(self, frame):
        # 1. ç”»åƒã®æº–å‚™
        img = frame.to_ndarray(format="bgr24")
        h, w, _ = img.shape
        
        # â˜… ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ç”¨ã®ã‚­ãƒ£ãƒ³ãƒã‚¹ã‚’ä½œæˆï¼ˆæ¨ªå¹…ã‚’åºƒã’ã‚‹ï¼‰
        # å…ƒã®ç”»åƒ(w) + å³å´ã®ãƒ‘ãƒãƒ«(300px)
        panel_w = 320
        canvas = np.zeros((h, w + panel_w, 3), dtype=np.uint8)
        
        # å·¦å´ã«ã‚«ãƒ¡ãƒ©æ˜ åƒã‚’ã‚³ãƒ”ãƒ¼
        canvas[:h, :w] = img

        # MediaPipeå‡¦ç†ï¼ˆå·¦å´ã®ç”»åƒã«å¯¾ã—ã¦è¡Œã†ï¼‰
        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        results = self.holistic.process(img_rgb)

        # ---------------------------------------------------------
        # 2. éª¨æ ¼æç”»ï¼ˆãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰ONã®æ™‚ã ã‘ï¼‰
        # ---------------------------------------------------------
        if self.debug:
            # ã‚­ãƒ£ãƒ³ãƒã‚¹ã®å·¦å´(ã‚«ãƒ¡ãƒ©éƒ¨åˆ†)ã«æç”»
            camera_area = canvas[:h, :w]
            mp_drawing.draw_landmarks(camera_area, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS)
            mp_drawing.draw_landmarks(camera_area, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS)
            mp_drawing.draw_landmarks(camera_area, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS)

        # ---------------------------------------------------------
        # 3. ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºã¨äºˆæ¸¬
        # ---------------------------------------------------------
        has_pose = results.pose_landmarks is not None
        has_lh = results.left_hand_landmarks is not None
        has_rh = results.right_hand_landmarks is not None
        
        self.status_text = f"P[{'O' if has_pose else 'X'}] L[{'O' if has_lh else 'X'}] R[{'O' if has_rh else 'X'}]"

        if model is not None:
            # æ­£è¦åŒ–ãƒ­ã‚¸ãƒƒã‚¯
            if has_pose:
                pose = np.array([[res.x, res.y, res.z] for res in results.pose_landmarks.landmark])
            else:
                pose = np.zeros((33, 3))
            if has_lh:
                lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark])
            else:
                lh = np.zeros((21, 3))
            if has_rh:
                rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark])
            else:
                rh = np.zeros((21, 3))

            if np.sum(pose) != 0:
                left_shoulder = pose[11]
                right_shoulder = pose[12]
                center = (left_shoulder + right_shoulder) / 2.0
                shoulder_width = np.linalg.norm(left_shoulder - right_shoulder)
                if shoulder_width < 0.01: shoulder_width = 1.0
            else:
                center = np.zeros(3)
                shoulder_width = 1.0

            pose_norm = (pose - center) / shoulder_width
            lh_norm = (lh - center) / shoulder_width
            rh_norm = (rh - center) / shoulder_width

            keypoints = np.concatenate([pose_norm.flatten(), lh_norm.flatten(), rh_norm.flatten()])
            self.sequence.append(keypoints)

            if len(self.sequence) == 30:
                input_data = np.expand_dims(list(self.sequence), axis=0)
                try:
                    prediction = model.predict(input_data, verbose=0)
                    self.probs = prediction[0] # å…¨ç¢ºç‡ã‚’ä¿å­˜
                    idx = np.argmax(self.probs)
                    self.result_conf = self.probs[idx]
                    
                    if idx < len(CLASS_NAMES):
                        self.result_label = CLASS_NAMES[idx]
                    else:
                        self.result_label = f"Class {idx}"

                except Exception:
                    pass

        # ---------------------------------------------------------
        # 4. ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰æç”»ï¼ˆå³å´ã®é»’ã„éƒ¨åˆ†ï¼‰
        # ---------------------------------------------------------
        # åŸºæº–ä½ç½® (å³å´ã®ãƒ‘ãƒãƒ«ã®é–‹å§‹ä½ç½®)
        x_start = w + 10
        y_cursor = 40
        font = cv2.FONT_HERSHEY_SIMPLEX
        
        # ã‚¿ã‚¤ãƒˆãƒ«ã‚¨ãƒªã‚¢
        cv2.putText(canvas, "AI Analysis", (x_start, y_cursor), font, 0.8, (255, 255, 255), 2)
        y_cursor += 40
        
        # ã‚»ãƒ³ã‚µãƒ¼çŠ¶æ³
        # è‰²ã‚’å¤‰ãˆã‚‹ (OKãªã‚‰ç·‘ã€NGãªã‚‰èµ¤)
        p_color = (0, 255, 0) if has_pose else (0, 0, 255)
        cv2.putText(canvas, self.status_text, (x_start, y_cursor), font, 0.5, p_color, 1)
        y_cursor += 40
        
        # åŒºåˆ‡ã‚Šç·š
        cv2.line(canvas, (w, y_cursor), (w+panel_w, y_cursor), (100, 100, 100), 1)
        y_cursor += 30

        # çµæœè¡¨ç¤º (å¤§ãã)
        cv2.putText(canvas, "Result:", (x_start, y_cursor), font, 0.6, (200, 200, 200), 1)
        y_cursor += 35
        # çµæœãƒ©ãƒ™ãƒ«ï¼ˆé»„è‰²ï¼‰
        cv2.putText(canvas, self.result_label, (x_start, y_cursor), font, 1.0, (0, 255, 255), 2)
        y_cursor += 30
        # ä¿¡é ¼åº¦
        cv2.putText(canvas, f"Conf: {self.result_conf*100:.1f}%", (x_start, y_cursor), font, 0.6, (0, 255, 255), 1)
        
        y_cursor += 40
        cv2.line(canvas, (w, y_cursor), (w+panel_w, y_cursor), (100, 100, 100), 1)
        y_cursor += 30

        # ã‚°ãƒ©ãƒ•æç”»ã‚¨ãƒªã‚¢
        cv2.putText(canvas, "Probabilities:", (x_start, y_cursor), font, 0.6, (200, 200, 200), 1)
        y_cursor += 20

        # å„ã‚¯ãƒ©ã‚¹ã®ãƒãƒ¼ã‚°ãƒ©ãƒ•
        bar_max_width = 180 # ãƒãƒ¼ã®æœ€å¤§é•·ã•
        for i, prob in enumerate(self.probs):
            class_name = CLASS_NAMES[i] if i < len(CLASS_NAMES) else str(i)
            
            # ã‚¯ãƒ©ã‚¹å
            y_cursor += 20
            cv2.putText(canvas, f"{class_name}", (x_start, y_cursor), font, 0.5, (255, 255, 255), 1)
            
            # ãƒãƒ¼ã®èƒŒæ™¯ï¼ˆã‚°ãƒ¬ãƒ¼ï¼‰
            y_bar = y_cursor + 5
            cv2.rectangle(canvas, (x_start, y_bar), (x_start + bar_max_width, y_bar + 10), (50, 50, 50), -1)
            
            # ãƒãƒ¼ã®ä¸­èº«ï¼ˆç¢ºç‡ã«å¿œã˜ã¦é•·ã•å¯å¤‰ã€è‰²ã¯é’ï¼‰
            bar_w = int(prob * bar_max_width)
            # äºˆæ¸¬ãƒˆãƒƒãƒ—ãªã‚‰è‰²ã‚’èµ¤ã«ã™ã‚‹ã€ãã‚Œä»¥å¤–ã¯ç·‘
            bar_color = (0, 0, 255) if prob == max(self.probs) else (0, 255, 0)
            
            if bar_w > 0:
                cv2.rectangle(canvas, (x_start, y_bar), (x_start + bar_w, y_bar + 10), bar_color, -1)
            
            # ãƒ‘ãƒ¼ã‚»ãƒ³ãƒˆæ•°å€¤
            cv2.putText(canvas, f"{prob*100:.0f}%", (x_start + bar_max_width + 10, y_bar + 8), font, 0.4, (200, 200, 200), 1)
            y_cursor += 20 # æ¬¡ã®è¡Œã¸

        return canvas

# ------------------------------------------------
# ã‚¢ãƒ—ãƒªç”»é¢æ§‹æˆ
# ------------------------------------------------
st.title("ğŸ¤Ÿ AIæ‰‹è©±è§£æã‚·ã‚¹ãƒ†ãƒ ")

if model is None:
    st.error("ãƒ¢ãƒ‡ãƒ«ãŒèª­ã¿è¾¼ã‚ã¾ã›ã‚“ã§ã—ãŸã€‚")
else:
    # WebRTCã®èµ·å‹•
    webrtc_streamer(
        key="sign-language-dashboard",
        video_processor_factory=VideoProcessor,
        rtc_configuration={"iceServers": [{"urls": ["stun:stun.l.google.com:19302"]}]},
        media_stream_constraints={"video": True, "audio": False},
    )
