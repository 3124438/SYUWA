import streamlit as st
import cv2
import numpy as np
import mediapipe as mp
import tensorflow as tf
from tensorflow.keras.layers import Layer
import tensorflow.keras.backend as K # â˜…è¿½åŠ ï¼šå­¦ç¿’ã‚³ãƒ¼ãƒ‰ã«åˆã‚ã›ã¦è¿½åŠ 
from streamlit_webrtc import webrtc_streamer, VideoTransformerBase
from collections import deque
import av

# =================================================
# âš™ï¸ è¨­å®šã‚¨ãƒªã‚¢
# =================================================
MODEL_FILE_NAME = "best_sign_model.keras"

# â˜…ã‚ãªãŸã®ã‚¯ãƒ©ã‚¹åã«åˆã‚ã›ã¦æ›¸ãæ›ãˆã¦ãã ã•ã„
CLASS_NAMES = ["Label 1", "Label 2", "Label 3", "å‹•ã‘ï¼ï¼"] 

# =================================================
# â˜…å­¦ç¿’ã‚³ãƒ¼ãƒ‰ã®ã€ŒAttentionã€ã‚’ãã®ã¾ã¾ç§»æ¤
# =================================================
@tf.keras.utils.register_keras_serializable()
class Attention(Layer):
    def __init__(self, **kwargs):
        super(Attention, self).__init__(**kwargs)

    def build(self, input_shape):
        self.W = self.add_weight(name='attention_weight', 
                                 shape=(input_shape[-1], 1), 
                                 initializer='normal', trainable=True)
        self.b = self.add_weight(name='attention_bias', 
                                 shape=(input_shape[1], 1), 
                                 initializer='zeros', trainable=True)
        super(Attention, self).build(input_shape)

    def call(self, x):
        # x: (batch_size, time_steps, features)
        e = K.tanh(K.dot(x, self.W) + self.b)
        a = K.softmax(e, axis=1) # æ™‚é–“è»¸ã«å¯¾ã—ã¦é‡ã¿ã‚’è¨ˆç®—
        output = x * a
        return K.sum(output, axis=1) # é‡ã¿ä»˜ãå’Œã‚’è¿”ã™

    def get_config(self):
        config = super(Attention, self).get_config()
        return config

# =================================================

@st.cache_resource
def load_model():
    return tf.keras.models.load_model(MODEL_FILE_NAME, custom_objects={'Attention': Attention})

try:
    model = load_model()
    st.success(f"ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿æˆåŠŸï¼: {MODEL_FILE_NAME}")
except Exception as e:
    st.error(f"ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
    model = None

# MediaPipeè¨­å®š
mp_holistic = mp.solutions.holistic

# ------------------------------------------------
# æ˜ åƒå‡¦ç†ã‚¯ãƒ©ã‚¹
# ------------------------------------------------
class VideoProcessor(VideoTransformerBase):
    def __init__(self):
        self.sequence = deque(maxlen=30)
        self.holistic = mp_holistic.Holistic(
            min_detection_confidence=0.5,
            min_tracking_confidence=0.5
        )
        self.prediction_text = "Waiting..."

    def transform(self, frame):
        img = frame.to_ndarray(format="bgr24")

        # 1. éª¨æ ¼æŠ½å‡º
        img.flags.writeable = False
        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        results = self.holistic.process(img_rgb)
        img.flags.writeable = True

        # 2. åº§æ¨™ãƒ‡ãƒ¼ã‚¿å¤‰æ›
        if model is not None:
            # â˜…ã“ã“ãŒä¿®æ­£ãƒã‚¤ãƒ³ãƒˆï¼å­¦ç¿’ã‚³ãƒ¼ãƒ‰ã€ŒFEATURES = 225ã€ã«åˆã‚ã›ã¾ã™
            # é †ç•ªé‡è¦: Pose(33) -> Left Hand(21) -> Right Hand(21)

            # (1) Pose (33ç‚¹ * 3 = 99æ¬¡å…ƒ)
            if results.pose_landmarks:
                pose = np.array([[res.x, res.y, res.z] for res in results.pose_landmarks.landmark]).flatten()
            else:
                pose = np.zeros(33*3)

            # (2) Left Hand (21ç‚¹ * 3 = 63æ¬¡å…ƒ)
            if results.left_hand_landmarks:
                lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten()
            else:
                lh = np.zeros(21*3)
            
            # (3) Right Hand (21ç‚¹ * 3 = 63æ¬¡å…ƒ)
            if results.right_hand_landmarks:
                rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten()
            else:
                rh = np.zeros(21*3)

            # å…¨éƒ¨ã¤ãªã’ã‚‹ (99 + 63 + 63 = 225æ¬¡å…ƒï¼)
            keypoints = np.concatenate([pose, lh, rh])
            self.sequence.append(keypoints)

            # 3. äºˆæ¸¬å®Ÿè¡Œ
            if len(self.sequence) == 30:
                input_data = np.expand_dims(list(self.sequence), axis=0)
                try:
                    prediction = model.predict(input_data, verbose=0)
                    predicted_index = np.argmax(prediction)
                    confidence = prediction[0][predicted_index]

                    if confidence > 0.7: # é–¾å€¤
                        if predicted_index < len(CLASS_NAMES):
                            self.prediction_text = f"{CLASS_NAMES[predicted_index]} ({confidence*100:.1f}%)"
                        else:
                            self.prediction_text = f"Class {predicted_index}"
                except Exception as e:
                    # æ¬¡å…ƒãŒåˆã‚ãªã„ç­‰ã®ã‚¨ãƒ©ãƒ¼ã¯ã“ã“ã§ç„¡è¦–ã•ã‚Œã‚‹ã®ã§ã€ä»Šå›žã¯printã§å‡ºã™ã‚ˆã†ã«ã—ã¦ã‚‚è‰¯ã„ã‹ã‚‚
                    print(f"Prediction Error: {e}")
                    pass

        # 4. æç”»
        cv2.rectangle(img, (0,0), (640, 40), (245, 117, 16), -1)
        cv2.putText(img, self.prediction_text, (10,30),
                    cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)

        return img

# ------------------------------------------------
# ã‚¢ãƒ—ãƒªç”»é¢æ§‹æˆ
# ------------------------------------------------
st.title("ðŸ¤Ÿ æ‰‹è©±ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ èªè­˜")
st.write(f"èª­ã¿è¾¼ã¿ãƒ¢ãƒ‡ãƒ«: {MODEL_FILE_NAME}")

webrtc_streamer(
    key="sign-language",
    video_processor_factory=VideoProcessor,
    rtc_configuration={
        "iceServers": [{"urls": ["stun:stun.l.google.com:19302"]}]
    },
    media_stream_constraints={"video": True, "audio": False},
)
