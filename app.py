import streamlit as st
import cv2
import numpy as np
import mediapipe as mp
import tensorflow as tf
from streamlit_webrtc import webrtc_streamer, VideoTransformerBase
from collections import deque
import av

# ------------------------------------------------
# 1. è¨­å®šãƒ»ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
# ------------------------------------------------
# â˜…é‡è¦â˜… ãƒ©ãƒ™ãƒ«ã‚’å­¦ç¿’ã•ã›ãŸé †ç•ªãƒ»å†…å®¹ã«åˆã‚ã›ã¦æ›¸ãæ›ãˆã¦ãã ã•ã„
LABELS = ['Label 1', 'Label 2', 'Label 3'] 

@st.cache_resource
def load_model():
    # ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ã‚¹ã‚’æŒ‡å®š
    return tf.keras.models.load_model('sign_language_model.h5')

try:
    model = load_model()
    st.success("ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿æˆåŠŸï¼")
except Exception as e:
    st.error(f"ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
    model = None

# MediaPipeè¨­å®š
mp_holistic = mp.solutions.holistic

# ------------------------------------------------
# 2. æ˜ åƒå‡¦ç†ã‚¯ãƒ©ã‚¹
# ------------------------------------------------
class VideoProcessor(VideoTransformerBase):
    def __init__(self):
        # 30ãƒ•ãƒ¬ãƒ¼ãƒ åˆ†ã®ãƒ‡ãƒ¼ã‚¿ã‚’è²¯ã‚ã‚‹ç®±
        self.sequence = deque(maxlen=30)
        self.holistic = mp_holistic.Holistic(
            min_detection_confidence=0.5,
            min_tracking_confidence=0.5
        )
        self.prediction_text = "Waiting..."

    def transform(self, frame):
        # WebRTCã‹ã‚‰ç”»åƒã‚’å–å¾—
        img = frame.to_ndarray(format="bgr24")

        # 1. éª¨æ ¼æŠ½å‡º (MediaPipe)
        img.flags.writeable = False
        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        results = self.holistic.process(img_rgb)
        img.flags.writeable = True

        # 2. éª¨æ ¼ã‚’ç”»é¢ã«æç”»ï¼ˆç¢ºèªç”¨ï¼šé‡ã‘ã‚Œã°ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆã—ã¦ãã ã•ã„ï¼‰
        # mp.solutions.drawing_utils.draw_landmarks(img, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS)
        # mp.solutions.drawing_utils.draw_landmarks(img, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS)

        # 3. åº§æ¨™ãƒ‡ãƒ¼ã‚¿å¤‰æ›
        if model is not None:
            # å·¦æ‰‹ãƒ»å³æ‰‹ã®æ¤œå‡º
            # â˜…æ³¨æ„â˜… å­¦ç¿’æ™‚ã®ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã¨å…¨ãåŒã˜ã«ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™
            if results.left_hand_landmarks:
                lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten()
            else:
                lh = np.zeros(21*3)
            
            if results.right_hand_landmarks:
                rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten()
            else:
                rh = np.zeros(21*3)

            # ãƒ‡ãƒ¼ã‚¿çµåˆã—ã¦ã‚­ãƒ¥ãƒ¼ã«è¿½åŠ  (å·¦æ‰‹+å³æ‰‹ = 126æ¬¡å…ƒ)
            keypoints = np.concatenate([lh, rh])
            self.sequence.append(keypoints)

            # 4. 30ãƒ•ãƒ¬ãƒ¼ãƒ æºœã¾ã£ãŸã‚‰äºˆæ¸¬ã‚’å®Ÿè¡Œ
            if len(self.sequence) == 30:
                # ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ€§ã‚’ä¿ã¤ãŸã‚ã€ãƒ‡ãƒ¼ã‚¿ã‚’æ•´ãˆã‚‹
                input_data = np.expand_dims(list(self.sequence), axis=0)
                
                try:
                    prediction = model.predict(input_data, verbose=0)
                    predicted_index = np.argmax(prediction)
                    confidence = prediction[0][predicted_index]

                    # ä¿¡é ¼åº¦ãŒ70%ä»¥ä¸Šã®ã¨ãã ã‘è¡¨ç¤ºæ›´æ–°
                    if confidence > 0.7:
                        # ç¯„å›²å¤–ã‚¨ãƒ©ãƒ¼ã‚’é˜²ã
                        if predicted_index < len(LABELS):
                            self.prediction_text = f"{LABELS[predicted_index]} ({confidence*100:.1f}%)"
                        else:
                            self.prediction_text = f"Class {predicted_index}"
                except Exception as e:
                    print(f"Prediction Error: {e}")

        # 5. çµæœã‚’ç”»é¢ã«æ›¸ãè¾¼ã‚€
        cv2.rectangle(img, (0,0), (640, 40), (245, 117, 16), -1)
        cv2.putText(img, self.prediction_text, (10,30),
                    cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)

        return img

# ------------------------------------------------
# 3. ã‚¢ãƒ—ãƒªç”»é¢æ§‹æˆ
# ------------------------------------------------
st.title("ğŸ¤Ÿ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ‰‹è©±åˆ¤å®š")
st.write("ã‚«ãƒ¡ãƒ©ã‚’è¨±å¯ã—ã¦ã€æ‰‹ã‚’å‹•ã‹ã—ã¦ãã ã•ã„ï¼ˆ30ãƒ•ãƒ¬ãƒ¼ãƒ è“„ç©å¾Œã«åˆ¤å®šã—ã¾ã™ï¼‰")

# WebRTCã®èµ·å‹•è¨­å®š
webrtc_streamer(
    key="sign-language",
    video_processor_factory=VideoProcessor,
    rtc_configuration={
        "iceServers": [{"urls": ["stun:stun.l.google.com:19302"]}]
    },
    media_stream_constraints={"video": True, "audio": False},
)
