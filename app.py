import streamlit as st
import cv2
import numpy as np
import mediapipe as mp
import tensorflow as tf
from tensorflow.keras.layers import Layer
import tensorflow.keras.backend as K
from streamlit_webrtc import webrtc_streamer, VideoTransformerBase
from collections import deque
import av

# =================================================
# âš™ï¸ è¨­å®šã‚¨ãƒªã‚¢
# =================================================
MODEL_FILE_NAME = "best_sign_model.keras"
CLASS_NAMES = ["Label 1", "Label 2", "Label 3","Label 4"] # ã‚ãªãŸã®ãƒ©ãƒ™ãƒ«ã«æ›¸ãæ›ãˆã¦ï¼

# =================================================
# Attentionå±¤
# =================================================
@tf.keras.utils.register_keras_serializable()
class Attention(Layer):
    def __init__(self, **kwargs):
        super(Attention, self).__init__(**kwargs)

    def build(self, input_shape):
        self.W = self.add_weight(name='attention_weight', 
                                 shape=(input_shape[-1], 1), 
                                 initializer='normal', trainable=True)
        self.b = self.add_weight(name='attention_bias', 
                                 shape=(input_shape[1], 1), 
                                 initializer='zeros', trainable=True)
        super(Attention, self).build(input_shape)

    def call(self, x):
        e = K.tanh(K.dot(x, self.W) + self.b)
        a = K.softmax(e, axis=1)
        output = x * a
        return K.sum(output, axis=1)

    def get_config(self):
        config = super(Attention, self).get_config()
        return config

@st.cache_resource
def load_model():
    return tf.keras.models.load_model(MODEL_FILE_NAME, custom_objects={'Attention': Attention})

try:
    model = load_model()
    st.success(f"ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿æˆåŠŸ: {MODEL_FILE_NAME}")
except Exception as e:
    st.error(f"ã‚¨ãƒ©ãƒ¼: {e}")
    model = None

# MediaPipeè¨­å®š
mp_holistic = mp.solutions.holistic
mp_drawing = mp.solutions.drawing_utils # â˜…æç”»ç”¨ãƒ„ãƒ¼ãƒ«

# ------------------------------------------------
# æ˜ åƒå‡¦ç†ã‚¯ãƒ©ã‚¹ï¼ˆãƒ‡ãƒãƒƒã‚°ãƒ•ãƒ«è£…å‚™ï¼‰
# ------------------------------------------------
class VideoProcessor(VideoTransformerBase):
    def __init__(self):
        self.sequence = deque(maxlen=30)
        self.holistic = mp_holistic.Holistic(
            min_detection_confidence=0.5,
            min_tracking_confidence=0.5
        )
        self.debug_text = "Initializing..."
        self.prob_text = ""

    def transform(self, frame):
        img = frame.to_ndarray(format="bgr24")

        img.flags.writeable = False
        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        results = self.holistic.process(img_rgb)
        img.flags.writeable = True

        # â˜… 1. éª¨æ ¼ã‚’ç”»é¢ã«æç”»ï¼ˆã“ã‚Œã§è¦‹ãˆã¦ã‚‹ã‹ç¢ºèªï¼ï¼‰
        mp_drawing.draw_landmarks(img, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS)
        mp_drawing.draw_landmarks(img, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS)
        mp_drawing.draw_landmarks(img, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS)

        # æ¤œå‡ºãƒ•ãƒ©ã‚°
        has_pose = results.pose_landmarks is not None
        has_lh = results.left_hand_landmarks is not None
        has_rh = results.right_hand_landmarks is not None

        if model is not None:
            # --- å­¦ç¿’ã‚³ãƒ¼ãƒ‰ã¨åŒã˜æ­£è¦åŒ–å‡¦ç† ---
            if has_pose:
                pose = np.array([[res.x, res.y, res.z] for res in results.pose_landmarks.landmark])
            else:
                pose = np.zeros((33, 3))

            if has_lh:
                lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark])
            else:
                lh = np.zeros((21, 3))
            
            if has_rh:
                rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark])
            else:
                rh = np.zeros((21, 3))

            # ç›¸å¯¾åº§æ¨™ãƒ»æ­£è¦åŒ–è¨ˆç®—
            if np.sum(pose) != 0:
                left_shoulder = pose[11]
                right_shoulder = pose[12]
                center = (left_shoulder + right_shoulder) / 2.0
                shoulder_width = np.linalg.norm(left_shoulder - right_shoulder)
                if shoulder_width < 0.01: shoulder_width = 1.0
            else:
                center = np.zeros(3)
                shoulder_width = 1.0

            pose_norm = (pose - center) / shoulder_width
            lh_norm = (lh - center) / shoulder_width
            rh_norm = (rh - center) / shoulder_width

            # çµåˆ
            keypoints = np.concatenate([pose_norm.flatten(), lh_norm.flatten(), rh_norm.flatten()])
            self.sequence.append(keypoints)

            # --- äºˆæ¸¬ ---
            if len(self.sequence) == 30:
                input_data = np.expand_dims(list(self.sequence), axis=0)
                try:
                    prediction = model.predict(input_data, verbose=0)
                    predicted_index = np.argmax(prediction)
                    confidence = prediction[0][predicted_index]

                    # â˜… é–¾å€¤ãªã—ã§ç”Ÿã®æ•°å­—ã‚’è¡¨ç¤º
                    label = CLASS_NAMES[predicted_index] if predicted_index < len(CLASS_NAMES) else str(predicted_index)
                    self.debug_text = f"Result: {label}"
                    self.prob_text = f"Conf: {confidence*100:.1f}%"
                    
                except Exception as e:
                    self.debug_text = "Error"
                    pass

        # â˜… ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã®æç”»
        # ã‚»ãƒ³ã‚µãƒ¼çŠ¶æ³ P=Pose, L=Left, R=Right
        status = f"P[{'O' if has_pose else 'X'}] L[{'O' if has_lh else 'X'}] R[{'O' if has_rh else 'X'}]"
        
        # é»’ã„å¸¯ã‚’å¼•ã„ã¦è¦‹ã‚„ã™ãã™ã‚‹
        cv2.rectangle(img, (0,0), (640, 90), (0, 0, 0), -1) 
        
        # æ–‡å­—ã‚’æ›¸ã
        cv2.putText(img, self.debug_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)
        cv2.putText(img, self.prob_text, (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)
        cv2.putText(img, status, (10, 85), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1)

        return img

st.title("ğŸ” å®Œå…¨ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰")
st.write("ä½“ã«ç·‘ã®ç·šãŒå‡ºã¦ã„ã‚‹ã‹ã€P[O]ã«ãªã£ã¦ã„ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„")

webrtc_streamer(
    key="sign-language-debug-final",
    video_processor_factory=VideoProcessor,
    rtc_configuration={"iceServers": [{"urls": ["stun:stun.l.google.com:19302"]}]},
    media_stream_constraints={"video": True, "audio": False},
)
